{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTlAdmUy03rK"
      },
      "source": [
        "## CNN for Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1fWo_d11RI6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnmOrxXo1M6N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "from tensorflow.keras import backend\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEnRmiif1RMB"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/Colab_Session/CNN/Brake_disk/'\n",
        "dataset_dir = base_path + 'Casting_Dataset/'\n",
        "\n",
        "if os.path.exists('/content/Casting_Dataset'):\n",
        "    !rm -rf '/content/Casting_Dataset'\n",
        "\n",
        "!unzip '/content/drive/MyDrive/Colab_Session/CNN/Brake_disk/Casting_Dataset.zip'\n",
        "\n",
        "if os.path.exists(dataset_dir):\n",
        "    !rm -rf '/content/drive/MyDrive/Colab_Session/CNN/Brake_disk/Casting_Dataset/'\n",
        "\n",
        "!cp -r '/content/Casting_Dataset' '/content/drive/MyDrive/Colab_Session/CNN/Brake_disk/Casting_Dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsmgQ4yp1gc5"
      },
      "outputs": [],
      "source": [
        "train_path = dataset_dir + 'Train/'\n",
        "test_path = dataset_dir + 'Test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P6hhsYC1qio"
      },
      "outputs": [],
      "source": [
        "image_gen = ImageDataGenerator(rescale=1/255,\n",
        "                               zoom_range=0.1,\n",
        "                               brightness_range=[0.9,1.0],\n",
        "                               validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mahrDft3KXv"
      },
      "outputs": [],
      "source": [
        "def load_image_dataset(root_dir, target_size=(100, 100), grayscale=True):\n",
        "    X = []\n",
        "    y = []\n",
        "    class_map = {}  # {'Train_Abnormal': 0, 'Train_Normal': 1}\n",
        "\n",
        "    for idx, class_name in enumerate(sorted(os.listdir(root_dir))):\n",
        "        class_path = os.path.join(root_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        class_map[class_name] = idx\n",
        "\n",
        "        for fname in os.listdir(class_path):\n",
        "            if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "                continue\n",
        "            img_path = os.path.join(class_path, fname)\n",
        "            img = Image.open(img_path)\n",
        "            if grayscale:\n",
        "                img = img.convert(\"L\")\n",
        "            else:\n",
        "                img = img.convert(\"RGB\")\n",
        "\n",
        "            img = img.resize(target_size)\n",
        "            X.append(np.array(img))\n",
        "            y.append(idx)\n",
        "\n",
        "    X = np.array(X)\n",
        "    if grayscale:\n",
        "        X = X[..., np.newaxis]\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y, class_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYzTNsLW3elo"
      },
      "outputs": [],
      "source": [
        "def get_tf_dataset(X, y, batch_size=32, shuffle=True):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(X))\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = torch.from_numpy(X).float().permute(0, 3, 1, 2)\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.X[idx], self.y[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPRiqkA3Rl7"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, class_map = load_image_dataset(train_path, target_size=(300, 300), grayscale=True)\n",
        "X_test, y_test, _ = load_image_dataset(test_path, target_size=(300, 300), grayscale=True)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n",
        "\n",
        "print(\"X shape:\", X_train.shape)\n",
        "print(\"y shape:\", y_train.shape)\n",
        "print(\"Classes:\", class_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9mKikE12lAA"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement a CNN model\n",
        "'''\n",
        "train_data: (n_train, 300, 300, 1)\n",
        "train_labels: (n_train, 2)\n",
        "Classes: {'Train_Abnormal': 0, 'Train_Normal': 1}\n",
        "\n",
        "Use the AI chatbot to help you. Build a CNN model for industrial data.\n",
        "You can import whatever you need. You can also define the model without a device (automatically selected)\n",
        "This time, the model can be huge, up to the limit allowed by Colab.\n",
        "'''\n",
        "'''Your code here'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL3h_jKu2lFN"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X_train, y_train, epochs=10, batch_size=32,\n",
        "                criterion=None, optimizer=None, loss='MSE', metrics=['accuracy'], lr=1e-3):\n",
        "\n",
        "    tf_gpu_available = tf.config.list_physical_devices('GPU')\n",
        "    torch_gpu_available = torch.cuda.is_available()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    if isinstance(model, tf.keras.Model):\n",
        "        device = \"GPU\" if tf_gpu_available else \"CPU\"\n",
        "        print(f\"[INFO] Detected TensorFlow model. Using {device}.\")\n",
        "\n",
        "        if optimizer is None:\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        "        )\n",
        "\n",
        "        train_dataset = get_tf_dataset(X_tr, y_tr, batch_size=batch_size, shuffle=True)\n",
        "        val_dataset = get_tf_dataset(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss=loss,\n",
        "                      metrics=metrics)\n",
        "\n",
        "        hist = model.fit(train_dataset,\n",
        "                         validation_data=val_dataset,\n",
        "                         epochs=epochs,\n",
        "                         batch_size=batch_size)\n",
        "\n",
        "        history['train_loss'] = hist.history.get('loss', [])\n",
        "        history['val_loss'] = hist.history.get('val_loss', [])\n",
        "        history['train_acc'] = hist.history.get('accuracy', [])\n",
        "        history['val_acc'] = hist.history.get('val_accuracy', [])\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    elif isinstance(model, torch.nn.Module):\n",
        "        device = torch.device(\"cuda\" if torch_gpu_available else \"cpu\")\n",
        "        print(f\"[INFO] Detected PyTorch model. Using {device}.\")\n",
        "        model.to(device)\n",
        "\n",
        "        full_dataset = NumpyDataset(X_train, y_train)\n",
        "        val_size = int(len(full_dataset) * 0.2)\n",
        "        train_size = len(full_dataset) - val_size\n",
        "        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        if optimizer is None:\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        if criterion is None:\n",
        "            criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss, correct, total = 0, 0, 0\n",
        "            for x_batch, y_batch in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{epochs}\"):\n",
        "                x_batch, y_batch = x_batch.to(device), y_batch.to(device).float().to(device)\n",
        "\n",
        "                y_scalar = y_batch.argmax(dim=1).long()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(x_batch)\n",
        "                loss = criterion(outputs, y_scalar)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item() * x_batch.size(0)\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == y_scalar).sum().item()\n",
        "                total += y_scalar.size(0)\n",
        "\n",
        "            train_acc = correct / total\n",
        "            history['train_loss'].append(total_loss / len(train_loader))\n",
        "            history['train_acc'].append(train_acc)\n",
        "\n",
        "            model.eval()\n",
        "            val_loss, val_correct, val_total = 0, 0, 0\n",
        "            with torch.no_grad():\n",
        "                for x_val, y_val in val_loader:\n",
        "                    x_val, y_val = x_val.to(device), y_val.to(device).float().to(device)\n",
        "\n",
        "                    y_scalar = y_val.argmax(dim=1).long()\n",
        "                    outputs = model(x_val)\n",
        "\n",
        "                    loss_v = criterion(outputs, y_scalar)\n",
        "                    val_loss += loss_v.item() * x_val.size(0)\n",
        "\n",
        "                    preds_v = outputs.argmax(dim=1)\n",
        "                    val_correct += (preds_v == y_scalar).sum().item()\n",
        "                    val_total += y_scalar.size(0)\n",
        "\n",
        "            val_acc = val_correct / val_total\n",
        "            history['val_loss'].append(val_loss / len(val_loader))\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            print(f\"[Train] Epoch {epoch+1}: Loss={history['train_loss'][-1]:.4f}, \"\n",
        "                  f\"Acc={train_acc:.4f} | \"\n",
        "                  f\"[Val] Loss={history['val_loss'][-1]:.4f}, Acc={val_acc:.4f}\")\n",
        "\n",
        "        return model, history\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Check the model type!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Wgx-LF2ad8"
      },
      "outputs": [],
      "source": [
        "model, history = train_model(model, X_train, y_train, epochs=5, batch_size=64, criterion=torch.nn.CrossEntropyLoss(), loss='binary_crossentropy', metrics=['accuracy'], lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6TonTzc6Yn2"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(model=None, history_dict=None):\n",
        "    if isinstance(model, tf.keras.Model):\n",
        "        if not hasattr(model, \"history\") or not hasattr(model.history, \"history\"):\n",
        "            raise ValueError(\"TensorFlow model does not have .history.history\")\n",
        "\n",
        "        history_df = pd.DataFrame(model.history.history)\n",
        "        history_df.index += 1\n",
        "        history_df.rename(columns={\n",
        "            'loss': 'train_loss',\n",
        "            'accuracy': 'train_acc',\n",
        "            'val_loss': 'val_loss',\n",
        "            'val_accuracy': 'val_acc'\n",
        "        }, inplace=True)\n",
        "\n",
        "    elif isinstance(model, torch.nn.Module):\n",
        "        if history_dict is None:\n",
        "            raise ValueError(\"For PyTorch, provide `history_dict` containing loss/accuracy lists.\")\n",
        "\n",
        "        history_df = pd.DataFrame(history_dict)\n",
        "        history_df.index += 1\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Model must be either a tf.keras.Model or torch.nn.Module\")\n",
        "\n",
        "    valid_cols = [c for c in ['train_loss', 'val_loss', 'train_acc', 'val_acc'] if c in history_df.columns]\n",
        "    if not valid_cols:\n",
        "        raise ValueError(\"No valid columns found in training history.\")\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.lineplot(data=history_df[valid_cols])\n",
        "    plt.title(\"TRAINING EVALUATION\", fontweight=\"bold\", fontsize=15)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Metrics\")\n",
        "    plt.legend(labels=valid_cols)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j75O04b36nSI"
      },
      "outputs": [],
      "source": [
        "plot_training_history(model, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qvq_oDe_Caw"
      },
      "outputs": [],
      "source": [
        "def evaluate_with_confusion_matrix(model, X_test, y_test):\n",
        "\n",
        "    if isinstance(model, tf.keras.Model):\n",
        "        print(\"[INFO] Detected TensorFlow model.\")\n",
        "        preds_proba = model.predict(X_test, verbose=0)\n",
        "        y_pred = np.argmax(preds_proba, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    elif isinstance(model, torch.nn.Module):\n",
        "        print(\"[INFO] Detected PyTorch model.\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        if X_test.ndim == 4:\n",
        "            X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
        "\n",
        "        X_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "        test_dataset = TensorDataset(X_tensor, y_tensor)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_loader:\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                output = model(x)\n",
        "                pred_label = output.argmax(dim=1).item()\n",
        "                y_pred.append(pred_label)\n",
        "\n",
        "                true_label = y.argmax(dim=1).item()\n",
        "                y_true.append(true_label)\n",
        "\n",
        "        y_pred = np.array(y_pred)\n",
        "        y_true = np.array(y_true)\n",
        "\n",
        "    else:\n",
        "        raise TypeError(\"Model must be a tf.keras.Model or torch.nn.Module\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='g', cmap='viridis', square=True, cbar=False, vmin=0)\n",
        "    plt.xticks(ticks=np.arange(2) + 0.5, labels=[\"abnormal\", \"normal\"])\n",
        "    plt.yticks(ticks=np.arange(2) + 0.5, labels=[\"abnormal\", \"normal\"])\n",
        "    plt.xlabel(\"PREDICT\")\n",
        "    plt.ylabel(\"ACTUAL\")\n",
        "    plt.title(\"CONFUSION MATRIX\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Confusion Matrix.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwPVbf09_Ief"
      },
      "outputs": [],
      "source": [
        "evaluate_with_confusion_matrix(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zg54Ko8_4u_"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model, test_cases, test_path, image_shape=(300, 300, 1)):\n",
        "\n",
        "    is_tf = isinstance(model, tf.keras.Model)\n",
        "    is_torch = isinstance(model, torch.nn.Module)\n",
        "\n",
        "    if is_torch:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if is_torch else None\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "\n",
        "    for i, rel_path in enumerate(test_cases):\n",
        "        full_path = os.path.join(test_path, rel_path)\n",
        "        img_gray = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img_gray = cv2.resize(img_gray, image_shape[:2])\n",
        "        img_rescaled = img_gray.astype(np.float32) / 255.0\n",
        "\n",
        "        if is_tf:\n",
        "            x_input = img_rescaled.reshape(1, *image_shape)\n",
        "            pred = model.predict(x_input, verbose=0)\n",
        "\n",
        "            if pred.shape[-1] == 2:\n",
        "                probs = tf.nn.softmax(pred, axis=1).numpy()\n",
        "            else:\n",
        "                probs = pred\n",
        "\n",
        "        elif is_torch:\n",
        "            x_input = torch.tensor(img_rescaled).unsqueeze(0).unsqueeze(0).float()\n",
        "            x_input = x_input.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                raw = model(x_input)\n",
        "                probs = torch.softmax(raw, dim=1).cpu().numpy()\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Unsupported model type.\")\n",
        "\n",
        "        img_rgb = cv2.imread(full_path)\n",
        "        img_rgb = cv2.resize(img_rgb, image_shape[:2])\n",
        "        img_rgb = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        label = rel_path.split(\"_\")[2]\n",
        "\n",
        "        pred_class = np.argmax(probs, axis=1)[0]\n",
        "        prob_percent = probs[0, pred_class] * 100\n",
        "\n",
        "        if pred_class == 0:\n",
        "            predicted_label = \"Abnormal\"\n",
        "        else:\n",
        "            predicted_label = \"Normal\"\n",
        "\n",
        "        plt.subplot(1, len(test_cases), i + 1)\n",
        "        plt.imshow(img_rgb, cmap='gray')\n",
        "        plt.title(f\"{rel_path.split('/')[1]}\\nActual Label: {label}\", weight='bold', size=14)\n",
        "        plt.xlabel(f\"Predicted: {predicted_label} ({prob_percent:.2f}%)\", color='g', size=16)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"correct_class_img.png\", dpi=500, bbox_inches='tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm5cUjMJ_5j7"
      },
      "outputs": [],
      "source": [
        "test_cases = ['Test_Normal/cast_ok_0_10.jpeg', 'Test_Abnormal/cast_def_0_26.jpeg']\n",
        "visualize_predictions(model, test_cases, test_path, image_shape=(300, 300, 1))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}